{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#1.Data_preparation"
   ],
   "metadata": {
    "id": "6DLxnOzAlciU",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Готовим датасеты для обучения\n",
    "\n",
    "Виды обработки текста: \n",
    "1) tokenization 2) stemming 3) lemmatization 4) stemming+misspelling 5) lemmatization+misspelling\n",
    "\n",
    "Виды векторизации: 1) bag of words 2) word counts 3) TFIDF 4) word2vec(бонус)\n"
   ],
   "metadata": {
    "id": "MbwOUYY5lkl-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "!pip install pymorphy3\n",
    "!pip install pattern\n",
    "!pip install pyspellchecker"
   ],
   "metadata": {
    "id": "bZRwj27xUH8j",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "33a596f6-21f0-4a27-db54-746f4e96ba23",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[1;32m      2\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstopwords\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpunkt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'nltk'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m1fKOor-6yp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.stem import PorterStemmer,  WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import pymorphy3\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Читаем данные из файла\n",
    "Собираем датафрейм, в котором колонка tweets содержит строки, колонка sentiments для классификации: 0 - негативный, 1 - нейтральный, 2 - позитивный"
   ],
   "metadata": {
    "id": "sjXNT8LCS387",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = DataFrame(columns = ['Tweets', 'Sentiments'])\n",
    "\n",
    "negative = pd.read_csv('p00_tweets/processedNegative.csv').T.reset_index()\n",
    "neutral = pd.read_csv('p00_tweets/processedNeutral.csv').T.reset_index()\n",
    "positive = pd.read_csv('p00_tweets/processedPositive.csv').T.reset_index()\n",
    "\n",
    "negative['Sentiments'] = 2\n",
    "neutral['Sentiments'] = 0\n",
    "positive['Sentiments'] = 1\n",
    "\n",
    "df = pd.concat([negative, neutral, positive]).reset_index(drop=True)\n",
    "df.columns=['Tweets','Sentiments']\n",
    "df\n"
   ],
   "metadata": {
    "id": "NCXyqRPL_Xlv",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "outputId": "53f087a3-8460-4bd0-e2fc-fc6b890110c1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mDataFrame\u001B[49m(columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTweets\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSentiments\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      3\u001B[0m negative \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mp00_tweets/processedNegative.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mreset_index()\n\u001B[1;32m      4\u001B[0m neutral \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mp00_tweets/processedNeutral.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mreset_index()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'DataFrame' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# почистим слова от тегов, ссылок и небуквенных символов\n",
    "def clean_text(text):\n",
    "  text=re.sub(r'@[A-Za-z0-9]+','',text) # removed @mentions\n",
    "  text=re.sub(r'#','',text) #removing symbol #\n",
    "  text=re.sub(r'RT[\\s]+','',text) #removing retweets\n",
    "  text=re.sub(r'https?:\\/\\/\\S+','',text) #removing hyperlinks\n",
    "  text=re.sub(r'[^A-Za-z\\s]','',text) #removing special characters\n",
    "  text=text.lower()\n",
    "  return text\n"
   ],
   "metadata": {
    "id": "rtgASM6phgG5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Токенизация и стоп-слова\n",
    "\n",
    "Токенизация – разбиение строки на кусочки – токены (мы разбиваем по словам).\n",
    "\n",
    "Stop Words - слова которые никак не влияются на содержание, типа “the, to, and, also”. Удаляем их.\n",
    "\n"
   ],
   "metadata": {
    "id": "d2gL8tS8yL6n",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# этот токенайзер удаляет пунктуацию, включая апострофы\n",
    "# после этого все стоп-слова норм убираются\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "def tokenize_text(text):\n",
    "  res = []\n",
    "  tokens = tokenizer.tokenize(text.lower())\n",
    "  for w in tokens:\n",
    "    if w not in stop_words:\n",
    "      res.append(w)\n",
    "  return clean_text(\" \".join(res))"
   ],
   "metadata": {
    "id": "YmGQ2tUEmoIS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['Tokens']=df['Tweets'].apply(tokenize_text)\n",
    "df\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "vSewnmnKh16l",
    "outputId": "683ab00b-39d1-4785-f085-33a5a10fd768",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Misspelling\n",
    "Коррекция грамматических ошибок\n"
   ],
   "metadata": {
    "id": "aPgmcnt1hftw",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def misspelling(text):\n",
    "  return(''.join(TextBlob(text).correct()))\n"
   ],
   "metadata": {
    "id": "Wx1iG93RiEF9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['Spell'] = df['Tokens'].apply(misspelling)\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "AXEm3se9tXAs",
    "outputId": "e429defd-5a75-4e5b-b77b-03d8e8894474",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Стемминг\n",
    "Оставляет у слова корень, обрезая окончания: making -> make, included -> includ\n",
    "\n",
    "Стемминг быстрее лемматизации"
   ],
   "metadata": {
    "id": "G8y2CdFXS2VX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus.reader import wordlist\n",
    "def stem_text(words):\n",
    "  res = []\n",
    "  stemmer = PorterStemmer()\n",
    "  for w in words.split():\n",
    "    res.append(stemmer.stem(w))\n",
    "  return(\" \".join(res))"
   ],
   "metadata": {
    "id": "kvrVZ6qyBiQs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['Stem']=df['Tokens'].apply(stem_text)\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "2LqdjwGbjmDp",
    "outputId": "69b6d803-557b-4050-b87e-32162c6d8ff3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['Stem_Spell']=df['Spell'].apply(stem_text)\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "outputId": "7f283b4e-8b02-4545-fe23-4bf4e1725ffa",
    "id": "g-UzB_N-CUqQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Лемматизация \n",
    "Возвращает лемму – базовую форму слова: better -> good, mice -> mouse.\n",
    "\n",
    "Лемматизация точнее стемминга, но медленнее, потому что требует обращаться к словарю"
   ],
   "metadata": {
    "id": "uTKttUmepIJS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#ntlk WordNetLemmatizer - хорошо работает если разметить типы слова (глагол, сущ, прилагательное)\n",
    "def get_wordnet_pos(tag):\n",
    "  from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
    "  if tag.startswith('J'):\n",
    "      return ADJ\n",
    "  elif tag.startswith('V'):\n",
    "      return VERB\n",
    "  elif tag.startswith('N'):\n",
    "      return NOUN\n",
    "  elif tag.startswith('R'):\n",
    "      return ADV\n",
    "  else:\n",
    "      return NOUN\n",
    "\n",
    "def lemmatize_text(words):\n",
    "  res = []\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  tagged = nltk.pos_tag(words.split()) # расставляет теги в словах для лучшей лемматизации\n",
    "  for w, tag in tagged:\n",
    "    res.append(lemmatizer.lemmatize(w, get_wordnet_pos(tag)))\n",
    "  return(' '.join(res))"
   ],
   "metadata": {
    "id": "XxAlgi6FomyO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['Lemm']=df['Tokens'].apply(lemmatize_text)\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "NcIfwq0LnjjP",
    "outputId": "db402288-2c1c-486d-e03d-70882a393207",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['Lemm_Spell']=df['Spell'].apply(lemmatize_text)\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "outputId": "33e0c133-de2d-4f30-f954-692482c76f5b",
    "id": "EUVtWtgNCdYL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#сохраним датафрейм в файл\n",
    "df.to_csv('/content/drive/MyDrive/p00_tweets/data/tweets.csv')"
   ],
   "metadata": {
    "id": "MRSE3GJaZyVH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Векторизация\n",
    "\n",
    "Переведем все предложения в набор цифр, чтобы на этих наборах обучать модель. "
   ],
   "metadata": {
    "id": "3cmvNPuiJ1Qo",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tweet1 = \"unhappi dog like though one\"\n",
    "tweet2 = \"miss go gig liverpool unhappi one\"\n",
    "tweet3 = \"isnt one new riverdal tonight unhappi\"\n",
    "text = [tweet1, tweet2, tweet3]\n",
    "text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcjVSTCenYmQ",
    "outputId": "8ed8c7db-f534-4de8-c75e-d8f560ff5396",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "binary_vect = CountVectorizer(binary=True)\n",
    "arr1 = binary_vect.fit_transform(text).toarray()\n",
    "arr1\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HKKoEA2LnkVf",
    "outputId": "aa8064f9-abe4-4a3d-cc17-db70c871ee98",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "count_vect = CountVectorizer(max_df=0.75)\n",
    "arr2 = count_vect.fit_transform(text).toarray()\n",
    "arr2"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "db58bcbe-96d5-464f-fe5a-7ce11c90135a",
    "id": "fldZz1Losi84",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tdidf_vect = TfidfVectorizer()\n",
    "arr3 = tdidf_vect.fit_transform(text).toarray()\n",
    "np.set_printoptions(precision=2)\n",
    "print(arr3)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJTBynODXgIV",
    "outputId": "649f586c-263a-47e7-803a-bf36b4054e6a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Параметры CountVectorizer():\n",
    "- binary=True - не принимается во внимание частота слова. Если слово встретилось - 1, если нет - 0\n",
    "- max_features - ограничение по максимальному количеству слов в словаре\n",
    "- max_df ограничивает максимальную частоту встречющегося слова - если слово слишком часто встречается, оно не несет информации, например \"he\". max_df=0.75 убирает из словаря слова, встречающиеся более чем в 75% твитов.\n",
    "- min_df ограничивает минимальную частоту встречаемого слова - если слово слишком редкое, то его можно игнорировать, например имена собственные. min_df=5 убирает из словаря слова, встречающиеся менее чем в 5 твитах.\n",
    "- ngram_range учитывает повторяющиеся последовательности слов в заданном диапазоне"
   ],
   "metadata": {
    "id": "vcT6p3Rytz0x",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0 or 1\n",
    "Создаем словарь из всех слов, которые есть в предложениях. Каждое предложение представляем в виде набора 0 и 1. 1 - если слово присутствует, 0 - остутствует"
   ],
   "metadata": {
    "id": "MpcR1zgbwIfQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def vectorize_01(col):\n",
    "  vectorizer = CountVectorizer(binary=True)\n",
    "  X = vectorizer.fit_transform(col).toarray()\n",
    "  # print(X.shape)\n",
    "  # print(X)\n",
    "  return X"
   ],
   "metadata": {
    "id": "YREP4GUAoKQy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X01=vectorize_01(df['Tokens'])\n",
    "X02=vectorize_01(df['Stem'])\n",
    "X03=vectorize_01(df['Lemm'])\n",
    "X04=vectorize_01(df['Stem_Spell'])\n",
    "X05=vectorize_01(df['Lemm_Spell'])\n"
   ],
   "metadata": {
    "id": "S08uU96Wx3go",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "YfQIAhTAsQ96",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Count words\n",
    "Учитывает частоту, с которой слово встречается в словаре. Благодяря этому можно отсеять самы частые и самые редкие слова и ограничить словарь"
   ],
   "metadata": {
    "id": "O-NEcshrOGak",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def vectorize_word_counts(col):\n",
    "  vectorizer = CountVectorizer(max_features=3000, min_df=5, max_df=0.7, ngram_range=(2, 4), analyzer='char_wb') \n",
    "  X = vectorizer.fit_transform(col).toarray()\n",
    "  # print(X.shape)\n",
    "  # print(X)\n",
    "  return X"
   ],
   "metadata": {
    "id": "kDsxdl_VET2c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X06=vectorize_word_counts(df['Tokens'])\n",
    "X07=vectorize_word_counts(df['Stem'])\n",
    "X08=vectorize_word_counts(df['Lemm'])\n",
    "X09=vectorize_word_counts(df['Stem_Spell'])\n",
    "X10=vectorize_word_counts(df['Lemm_Spell'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2QLyuz-ng0JX",
    "outputId": "fbb0d470-b110-4a26-cf19-a28a38b933bc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##TFIDF\n",
    "Считает отношение частоты встречаемости слов к общему числу слов в документе. Таким образом, каждому слову присваивается \"вес\" который будет учитываться при обучении модели"
   ],
   "metadata": {
    "id": "5ysWYd5xJfSt",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def vectorize_tdidf(col):\n",
    "  tfidfconverter = TfidfVectorizer(ngram_range=(1,2))\n",
    "  X = tfidfconverter.fit_transform(col).toarray()\n",
    "  # print(X.shape)\n",
    "  # print(X)\n",
    "  return X"
   ],
   "metadata": {
    "id": "dWbSlyh_AEkG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X11=vectorize_tdidf(df['Tokens'])\n",
    "X12=vectorize_tdidf(df['Stem'])\n",
    "X13=vectorize_tdidf(df['Lemm'])\n",
    "X14=vectorize_tdidf(df['Stem_Spell'])\n",
    "X15=vectorize_tdidf(df['Lemm_Spell'])"
   ],
   "metadata": {
    "id": "WdlD7rE6xz25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Train test split"
   ],
   "metadata": {
    "id": "xRSjpWUT4oPe",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y = df['Sentiments']"
   ],
   "metadata": {
    "id": "qKfNcucNQHDv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train, y_test = train_test_split(y, test_size=0.2, stratify=df['Sentiments'], random_state=42)\n",
    "X01_train, X01_test = train_test_split(X01, test_size=0.2, stratify=df['Sentiments'], random_state=42) # добавить стратификацию\n",
    "X02_train, X02_test = train_test_split(X02, test_size=0.2, random_state=42)\n",
    "X03_train, X03_test = train_test_split(X03, test_size=0.2, random_state=42)\n",
    "X04_train, X04_test = train_test_split(X04, test_size=0.2, random_state=42)\n",
    "X05_train, X05_test = train_test_split(X05, test_size=0.2, random_state=42)\n",
    "X06_train, X06_test = train_test_split(X06, test_size=0.2, random_state=42)\n",
    "X07_train, X07_test = train_test_split(X07, test_size=0.2, random_state=42)\n",
    "X08_train, X08_test = train_test_split(X08, test_size=0.2, random_state=42)\n",
    "X09_train, X09_test = train_test_split(X09, test_size=0.2, random_state=42)\n",
    "X10_train, X10_test = train_test_split(X10, test_size=0.2, random_state=42)\n",
    "X11_train, X11_test = train_test_split(X11, test_size=0.2, random_state=42)\n",
    "X12_train, X12_test = train_test_split(X12, test_size=0.2, random_state=42)\n",
    "X13_train, X13_test = train_test_split(X13, test_size=0.2, random_state=42)\n",
    "X14_train, X14_test = train_test_split(X14, test_size=0.2, random_state=42)\n",
    "X15_train, X15_test = train_test_split(X15, test_size=0.2, random_state=42)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9iZ3i6Z1fDr",
    "outputId": "cc5952e1-4732-47e0-db18-d7115185e798",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Random forest"
   ],
   "metadata": {
    "id": "vo1aNiZL76wm",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "results = DataFrame(index = ['tokenization', 'stemming', 'lemmatization', 'stemming+misspelling', 'lemmatization+misspelling'], \n",
    "                    columns=['0or1', 'word_counts', 'TFIDF', 'word2vec'])\n"
   ],
   "metadata": {
    "id": "yIAuPFT6Kn5l",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=200, random_state=0)"
   ],
   "metadata": {
    "id": "TOrAhKyF72-l",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X01_train, y_train)\n",
    "y_pred = classifier.predict(X01_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy01 = accuracy_score(y_test, y_pred)\n",
    "results.loc['tokenization', '0or1'] = accuracy01"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hLGSSup6nu1",
    "outputId": "06f5d5be-c4d5-45b4-a203-d6ce09adf76c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X02_train, y_train)\n",
    "y_pred = classifier.predict(X02_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy02 = accuracy_score(y_test, y_pred)\n",
    "results.loc['stemming', '0or1'] = accuracy02"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "994dd796-88e4-4282-bdb1-3dc947c1084d",
    "id": "9pvLqcuv7cHK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X03_train, y_train)\n",
    "y_pred = classifier.predict(X03_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy03 = accuracy_score(y_test, y_pred)\n",
    "results.loc['lemmatization', '0or1'] = accuracy03"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jalVhtu97viS",
    "outputId": "f8f1da95-8ec6-43b7-e999-3fb19f020767",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X04_train, y_train)\n",
    "y_pred = classifier.predict(X04_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy04 = accuracy_score(y_test, y_pred)\n",
    "results.loc['stemming+misspelling', '0or1'] = accuracy04"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a4052cfc-ade9-46d2-84b4-14d19d95f96b",
    "id": "3oYiqRol8XYI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X05_train, y_train)\n",
    "y_pred = classifier.predict(X05_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy05 = accuracy_score(y_test, y_pred)\n",
    "results.loc['lemmatization+misspelling', '0or1'] = accuracy05"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "36a75275-0920-4fae-94f4-7765272761ca",
    "id": "pPICfJvW8o0j",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X06_train, y_train)\n",
    "y_pred = classifier.predict(X06_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy06 = accuracy_score(y_test, y_pred)\n",
    "results.loc['tokenization', 'word_counts'] = accuracy06"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "992959cb-ee3d-4f7a-bb55-5c88fc7225c8",
    "id": "D7si_DV8-mLf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X07_train, y_train)\n",
    "y_pred = classifier.predict(X07_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy07 = accuracy_score(y_test, y_pred)\n",
    "results.loc['stemming', 'word_counts'] = accuracy07"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bc1c09dd-a8dd-4b3e-cb9c-1cff727a09ed",
    "id": "7PiB8pGH-mLf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X08_train, y_train)\n",
    "y_pred = classifier.predict(X08_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy08 = accuracy_score(y_test, y_pred)\n",
    "results.loc['lemmatization', 'word_counts'] = accuracy08"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "359c3e4c-2697-4adb-c086-c173e030a95e",
    "id": "-SMRzp2J-mLg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X09_train, y_train)\n",
    "y_pred = classifier.predict(X09_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy09 = accuracy_score(y_test, y_pred)\n",
    "results.loc['stemming+misspelling', 'word_counts'] = accuracy09"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2bcbb994-a50c-4c47-dccd-86cf84bcd615",
    "id": "gA0UXxVR-mLg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X10_train, y_train)\n",
    "y_pred = classifier.predict(X10_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy10 = accuracy_score(y_test, y_pred)\n",
    "results.loc['lemmatization+misspelling', 'word_counts'] = accuracy10"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7996e832-7811-4136-97c8-20558d23591d",
    "id": "UrBkwslh-mLg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X11_train, y_train)\n",
    "y_pred = classifier.predict(X11_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy11 = accuracy_score(y_test, y_pred)\n",
    "results.loc['tokenization', 'TFIDF'] = accuracy11"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "be4d11ed-a4a6-4ae5-a3ac-67f04a57f16b",
    "id": "fGUJN7Vf_gv3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X12_train, y_train)\n",
    "y_pred = classifier.predict(X12_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy12 = accuracy_score(y_test, y_pred)\n",
    "results.loc['stemming', 'TFIDF'] = accuracy12"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7c64af5e-f525-4b43-87eb-3cee8170a1d1",
    "id": "V9UwXc6m_gv_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X13_train, y_train)\n",
    "y_pred = classifier.predict(X13_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy13 = accuracy_score(y_test, y_pred)\n",
    "results.loc['lemmatization', 'TFIDF'] = accuracy13"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "00e652c8-4777-4c76-fa56-c5a11264b0ad",
    "id": "4Ts6Irne_gv_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X14_train, y_train)\n",
    "y_pred = classifier.predict(X14_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy14 = accuracy_score(y_test, y_pred)\n",
    "results.loc['stemming+misspelling', 'TFIDF'] = accuracy14"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fa89bdd5-8595-44fc-f7b2-61c40dec007e",
    "id": "oOsj24ak_gv_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(X15_train, y_train)\n",
    "y_pred = classifier.predict(X15_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "accuracy15 = accuracy_score(y_test, y_pred)\n",
    "results.loc['lemmatization+misspelling', 'TFIDF'] = accuracy15"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "93280a60-dbf7-412e-959f-04ccf973e65c",
    "id": "RnUhJz-9_gv_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "results"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "uZx7PWDnO7ME",
    "outputId": "b09bc2e6-22ad-4ad0-f29e-ee4f14997608",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "План\n",
    "- ✔ (работает, но оооооч медленно) misspelling - Ира\n",
    "- ✅ разобраться с параметрами в векторизации ф-ция CountVectorizer - Ира - вот здесь хорошо описано: https://itchef.ru/articles/51879/\n",
    "- попробовать сохранить массив в файл и прочитать его по-нормальному - Юля\n",
    "- сделать ф-цию которая будет делать сплит трейн тест - Ира\n",
    "- штука которая сравнивает результаты - Юля\n",
    "- подумать как все датасеты загнать в модель - Юля, Ира\n",
    "- посмотреть другие модели - Юля\n",
    "- vord to vec - Юля, Ира\n",
    "- 10 наиболее совпадающих твитов в каждом датасете (массив numpy) - Юля, Ира"
   ],
   "metadata": {
    "id": "O2TKDE9LsTZm",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}